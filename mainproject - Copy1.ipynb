{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "12322162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linkGenerator(word):\n",
    "    link='https://news.google.com/rss/search?q='\n",
    "    link+=word\n",
    "    return link\n",
    "#     print(link)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bec5e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def xmlparse(link):\n",
    "    tree = ET.parse(link)\n",
    "# print(type(tree))\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    title=[]\n",
    "    link=[] \n",
    "    desc=[]\n",
    "    source=[]\n",
    "    date=[]\n",
    "    for child in root:\n",
    "        for i in range(8,len(child)):\n",
    "            for j in range(len(child[i])):\n",
    "                if child[i][j].tag=='title':\n",
    "                    title.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='link':\n",
    "                    link.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='description':\n",
    "                    desc.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='source':\n",
    "                    source.append(child[i][j].text)\n",
    "                elif child[i][j].tag=='pubDate':\n",
    "                    date.append(child[i][j].text)\n",
    "    dict={'title':title,'link':link,'pubdate':date,'source':source}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "01e2f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def businessToday(link):\n",
    "    response=requests.get(link)\n",
    "    author=''\n",
    "    soup=BeautifulSoup(response.text,'html.parser')   \n",
    "    content=soup.find('div',{'class':'content-area'})\n",
    "    try:\n",
    "        heading=content.find('div',{'class':'story-heading'}).text\n",
    "    except:\n",
    "        heading='No heading'\n",
    "    #     print(heading)\n",
    "    try:\n",
    "        subheading=content.find('div',{'class':'sab-head-tranlate-sec'}).text\n",
    "    except:\n",
    "        subheading='No subheading'\n",
    "        #     print(subheading)\n",
    "    try:\n",
    "        userdetail=content.find('div',{'class':'userdetail-share-main story-user-section'})\n",
    "        left=userdetail.find('div',{'class':'user-detial-left'})\n",
    "        branddetail=left.find('div',{'class':'brand-detial-main'})\n",
    "        date=branddetail.find('ul').text\n",
    "        date=date.replace('Updated ','')\n",
    "    except:\n",
    "        date='No date'\n",
    "        #     print(date)\n",
    "    try:\n",
    "        story=content.find('div',{'class':'stroy-870'})\n",
    "        main=story.find('div',{'class':'story-with-main-sec'})\n",
    "        storyFormat=main.find('div',{'class':'text-formatted field field--name-body field--type-text-with-summary field--label-hidden field__item'})\n",
    "        p=storyFormat.find_all('p')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news='No data'\n",
    "            #     print(data)\n",
    "    dict={'Source':['Business Today'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[date],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "871e4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamic class\n",
    "def economicTimes(link):\n",
    "    author=''\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'article_wrap'})\n",
    "    headline=content.find('div',{'class':'topPart clearfix tac fixedOnLoad'})\n",
    "    heading=headline.find('h1').text\n",
    "    subheading=''\n",
    "#     print(heading)\n",
    "    metadata=content.find('div',{'class':'bylineBox'})\n",
    "    contentwrapper=metadata.find('div',{'class':'dt contentwrapper'})\n",
    "    artbyline=contentwrapper.find('div',{'class':'dtc vam artByline'})\n",
    "    time=artbyline.find('time',{'class':'jsdtTime'}).text\n",
    "    time=time.replace('Last Updated: ','')\n",
    "#     print(time)\n",
    "#     author=artbyline.find('div',{'class':'auth eventDone'})\n",
    "#     a_nam=author.find('a').text\n",
    "#     print(a_name)\n",
    "    news=[]\n",
    "    newsdata=content.find('div',{'class':'edition clearfix'})\n",
    "    pagecontent=newsdata.find('div',{'class':'pageContent flt'})\n",
    "    article=pagecontent.find('article',{'class':'artData clr paywall'}).text\n",
    "#     p=article.find('p').text\n",
    "#     print(article)\n",
    "#     arttext=article.find('div',{'class':'artText medium'}).text\n",
    "#     br=arttext.find('em')\n",
    "#     for i in br:\n",
    "#         news.append(i.text)\n",
    "#     print(br)\n",
    "    dict={'Source':['Economic Times'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[time],'news':[article]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27629ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('section',{'class':'mainSec'})\n",
    "    try:\n",
    "        head=content.find('div',{'class':'stickyCare'})\n",
    "        heading=head.find('h1',{'class':'headline'}).text\n",
    "    except:\n",
    "        heading='no heading'\n",
    "    data=[]\n",
    "    subheading=''\n",
    "#     print(heading)\n",
    "    try:\n",
    "        publish=head.find('span',{'class':'articleInfo pubtime'})\n",
    "        lastUpdate=publish.find_all('span')[1].text\n",
    "        lastUpdate=lastUpdate.replace('Updated: ','')\n",
    "    except:\n",
    "        lastUpdate='no date'\n",
    "        #     print(lastUpdate)\n",
    "    try:\n",
    "        author=head.find('span',{'class':'articleInfo author'})\n",
    "        name=author.find_all('a')\n",
    "        for i in name:\n",
    "            a_name=i.find('strong').text\n",
    "#             a_name=i.text\n",
    "    except:\n",
    "        a_name='No author'\n",
    "\n",
    "#     print(a_name)\n",
    "    try:\n",
    "        news=content.find('div',{'class':'contentSec'})\n",
    "        first=news.find('div',{'class':'FirstEle'})\n",
    "        data.append(first.text)\n",
    "    #     print(first.text)\n",
    "        second=news.find('div',{'class':'paywall'})\n",
    "        p=second.find_all('p');\n",
    "        for i in p:\n",
    "            data.append(i.text)\n",
    "    except:\n",
    "        data='No data'\n",
    "#         print(i.text)\n",
    "    dict={'Source':['Mint'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[lastUpdate],'news':[data]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c18b81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def businessStandard(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'story-detail'})\n",
    "    author=''\n",
    "    try:\n",
    "        heading=content.find('h1').text\n",
    "    except:\n",
    "        heading='no heading'\n",
    "#     print(heading.text)\n",
    "    try:\n",
    "        subheading=content.find('h2').text\n",
    "    except:\n",
    "        subheading='no sub'\n",
    "#     print(subheading.text)\n",
    "    try:\n",
    "        data=content.find('div',{'class':'storycontent'})\n",
    "        p=data.find_all('p')\n",
    "        if p==[]:\n",
    "            p=data.find_all('div')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news='no news'\n",
    "            #     print(news)\n",
    "    try:\n",
    "        date=content.find('div',{'class':'story-first-time'})\n",
    "        p=date.find('p').text\n",
    "        p=p.replace('First Published: ','')\n",
    "    except:\n",
    "        p='no date'\n",
    "        #     print(p)\n",
    "    dict={'Source':['Business Standard'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[p],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "50729f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moneyControl(link):\n",
    "    response=requests.get(link)\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'page_left_wrapper'})\n",
    "    try:\n",
    "        heading=content.find('h1',{'class':'article_title artTitle'}).text\n",
    "    except:\n",
    "        heading=''\n",
    "        #     print(heading)\n",
    "    try:\n",
    "        subheading=content.find('h2',{'class':'article_desc'}).text\n",
    "    except:\n",
    "        subheading=''\n",
    "        #     print(subheading)\n",
    "    try:   \n",
    "        metadata=content.find('div',{'class':'clearfix'})\n",
    "        author=metadata.find('div',{'class':'article_author'}).text\n",
    "        author=author\n",
    "    except:\n",
    "        author=''\n",
    "        #     print(author.strip())\n",
    "    try:\n",
    "        dates=metadata.find('div',{'class':'article_schedule'}).text\n",
    "        dates=dates\n",
    "    except:\n",
    "        dates=''\n",
    "#     print(dates.strip())\n",
    "    try:\n",
    "        data=content.find('div',{'class':'content_wrapper arti-flow'})\n",
    "        p=data.find_all('p')\n",
    "        news=[]\n",
    "        for i in p:\n",
    "            news.append(i.text)\n",
    "    except:\n",
    "        news=''\n",
    "#     print(news)\n",
    "    dict={'Source':['Money Control'],'heading':[heading],'subheading':[subheading],'author':[author.strip()],'date':[dates.strip()],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb60f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toi(link):\n",
    "    response=requests.get(link)\n",
    "    author=''\n",
    "    subheading=''\n",
    "    soup=BeautifulSoup(response.text,'html.parser')\n",
    "    content=soup.find('div',{'class':'okf2Z'})\n",
    "    try:\n",
    "#         print('hejjs')\n",
    "        heading=content.find('div',{'class':'pZFl7'}).text\n",
    "    except:\n",
    "        heading=''\n",
    "        #     print(heading.text)\n",
    "    try:\n",
    "        time=heading.find('div',{'class':'t8vf3 byline_action'})\n",
    "        date=time.find('span').text\n",
    "    except:\n",
    "        date=''\n",
    "#     print(date)\n",
    "    data=content.find('div',{'class':'JuyWl'})\n",
    "    data1=data.find('div',{'class':'vSlIC'})\n",
    "    data2=data1.find('div',{'class':'heightCalc'})\n",
    "    x=soup.find_all('div')\n",
    "    for i in x:\n",
    "        y=i.get('class')\n",
    "        if 'fewcent' in str(y):\n",
    "            classs=y\n",
    "    data3=data2.find('div',{'class':classs})\n",
    "    news=data3.find('div',{'class':'_s30J clearfix'}).text\n",
    "#     news=''\n",
    "        #     print(news)\n",
    "    dict={'Source':['TOI'],'heading':[heading],'subheading':[subheading],'author':[author],'date':[date],'news':[news]}\n",
    "    df=pd.DataFrame(dict)\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4416dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmlSaver(link):\n",
    "    import requests\n",
    "\n",
    "#     URL = \"https://news.google.com/rss/search?q=NIFTY\"\n",
    "    URL=link\n",
    "\n",
    "    response = requests.get(URL)\n",
    "    with open('file.xml', 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "508fdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import requests \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def scrappers(df,index):\n",
    "    source=df['source'][index]\n",
    "    if source=='Mint':\n",
    "        dataframe=mint(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Business Standard':\n",
    "        dataframe=businessStandard(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Economic Times':\n",
    "        dataframe=economicTimes(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Business Today':\n",
    "        dataframe=businessToday(df['link'][index])\n",
    "        return dataframe\n",
    "    elif source=='Times of India':\n",
    "        dataframe=toi(df['link'][index])\n",
    "    elif source=='Moneycontrol':\n",
    "        dataframe=moneyControl(df['link'][index])    \n",
    "#     else:\n",
    "#         print('working on it!!!')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "245febc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    word=input(\"Enter stock symbol to scrap\\n\")\n",
    "    link=linkGenerator(word)\n",
    "    file=xmlSaver(link)\n",
    "#     print(file)\n",
    "#     df=xmlparse('C:\\\\Users\\\\PadamShree\\\\Desktop\\\\project 6th sem\\\\file.xml')\n",
    "    from pathlib import Path\n",
    "    link=Path.home()\n",
    "    link=str(link)+'\\\\Downloads\\\\file.xml'\n",
    "    df=xmlparse(link)\n",
    "#     df=link\n",
    "    counts=df['source'].value_counts()\n",
    "\n",
    "\n",
    "    temp_dict={}\n",
    "    df_all=pd.DataFrame(temp_dict)\n",
    "    for i in range(0,100):\n",
    "        if df['source'][i]=='Economic Times' :\n",
    "            continue\n",
    "#         df_all.append(scrappers(df,i))\n",
    "#     return df_all\n",
    "        \n",
    "        dict=scrappers(df,i)\n",
    "#         print(dict)\n",
    "        df_all=pd.concat([dict,df_all])\n",
    "    return df_all\n",
    "#     scrappers(df,0)\n",
    "#     scrappers(df,1)\n",
    "#     scrappers(df,2)\n",
    "#     scrappers(df,3)\n",
    "#     df=scrappers(df,4)\n",
    "#     print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ce16dd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter stock symbol to scrap\n",
      "tcs\n"
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    df = main()\n",
    "#     df = df.transpose()\n",
    "    df.to_csv('main.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "dea1c5b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6ccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd27a9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
